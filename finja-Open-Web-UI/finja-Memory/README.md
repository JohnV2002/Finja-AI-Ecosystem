# ğŸ“š Finja Cloud Memory v1.3.1

Ein leichtgewichtiger, blitzschneller und externer **Memory-Service**, der als LangzeitgedÃ¤chtnis fÃ¼r KI-Projekte wie Finja dient. Dieses System ist fÃ¼r die nahtlose Integration mit **OpenWebUI** Ã¼ber das `adaptive_memory_v4` Plugin konzipiert.

---

## ğŸš¨ Wichtiger Hinweis: Externer Server Zwingend Erforderlich!

Dieses System besteht aus zwei Teilen: dem **Server** (dieses Repository) und dem **Plugin**. Das Plugin funktioniert **NICHT** ohne den hier beschriebenen Memory-Server.

> Bitte folge zuerst der Setup-Anleitung, um den Server via Docker zu starten, bevor du das Plugin in OpenWebUI installierst.

---

## âœ¨ Features

### Server (`memory-server.py` - v1.3.1)
-   **Intelligenter RAM-Cache:** HÃ¤lt aktive User-Daten im Arbeitsspeicher fÃ¼r blitzschnelle Lesezugriffe und gibt den Speicher nach einer Zeit der InaktivitÃ¤t automatisch wieder frei.
-   **Persistente Speicherung:** Sichert alle Erinnerungen als portable JSON-Dateien pro Benutzer in einem Docker-Volume.
-   **Voice-Memory-GerÃ¼st:** Bietet API-Endpunkte zur Annahme von Sprachdateien (`/add_voice_memory`) und zum Caching von Sprachausgaben (`/get_or_create_speech`), vorbereitet fÃ¼r STT/TTS-Modelle.
-   **Datenkontrolle:** EnthÃ¤lt einen API-Endpunkt (`/delete_user_memories`), der es dem Plugin ermÃ¶glicht, alle Daten eines Benutzers auf Anfrage sicher und vollstÃ¤ndig zu lÃ¶schen.
-   **Sicherheit:** Der Zugriff wird Ã¼ber einen `X-API-Key` in einer `.env`-Datei abgesichert.
-   **Backup-Endpunkte:** EnthÃ¤lt `/backup_all_now` (Admin) zum Sichern aller Daten und `/backup_now` (Platzhalter fÃ¼r User-Backups).

### Plugin (`adaptive_memory_v4.py` - v4.3.11)
-   **Flexible Provider-Wahl:**
    -   **Extraktion:** WÃ¤hle zwischen OpenAI (`openai`) und einem lokalen LLM (`local`, z.B. Ollama).
    -   **Relevanz:** WÃ¤hle zwischen OpenAI (`openai`), lokalem LLM (`local`) oder rein lokalen Embeddings (`embedding`).
    -   **Lokale Embeddings:** WÃ¤hle zwischen der `sentence-transformers`-Bibliothek (`sentence_transformer`) oder der Ollama Embeddings API (`ollama`).
-   **Intelligente Extraktion:** Nutzt den konfigurierten LLM, um aus GesprÃ¤chen dauerhafte Fakten zu extrahieren und dabei von einmaligen Ereignissen zu generalisieren.
-   **Performance & Kosten-Optimierung:**
    -   Ein **"Themen-Cache"** vermeidet unnÃ¶tige API-Anfragen, solange das GesprÃ¤chsthema gleich bleibt (nutzt lokale Embeddings).
    -   Eine **lokale Vor-Filterung** (nutzt lokale Embeddings) reduziert die Anzahl der an den LLM fÃ¼r die RelevanzprÃ¼fung gesendeten Erinnerungen drastisch.
-   **Robuste Duplikats-Erkennung:** Verwendet eine mehrstufige PrÃ¼fung (Cosine Similarity via OpenAI oder lokalem Embedding & Levenshtein-Distanz), um doppelte Erinnerungen zu blockieren.
-   **Fallback-System:** Nutzt lokale Embeddings als Fallback fÃ¼r Relevanz/Deduplikation, wenn der ausgewÃ¤hlte LLM-Provider fehlschlÃ¤gt.
-   **Benutzerfreundlichkeit:**
    -   Ein **Server-Verbindungs-Check** gibt beim Start eine klare Fehlermeldung, falls der Server nicht erreichbar ist.
    -   **Klares User-Feedback** im Chat informiert Ã¼ber alle Aktionen des Plugins.
    -   Eine **Zwei-Stufen-BestÃ¤tigung** per Chat-Befehl ermÃ¶glicht dem User, die LÃ¶schung seiner Daten selbst zu steuern.
-   **StabilitÃ¤t:** EnthÃ¤lt diverse Bugfixes fÃ¼r Fehlerbehandlung, Provider-Logik und Statusmeldungen.

---

## ğŸš€ Setup mit Docker Compose (Empfohlen)

Dies ist die einfachste und sicherste Methode, den Server zu starten.

### 1. Konfigurationsdatei erstellen
Erstelle im Hauptverzeichnis eine `.env`-Datei. Hier werden deine geheimen API-Keys gespeichert.

```ini
# .env
MEMORY_API_KEY="dein-super-sicherer-key-12345"
# OPENAI_API_KEY="sk-dein-openai-key" # Optional, nur wenn OpenAI als Provider genutzt wird
```
> âš ï¸ **Wichtig:** FÃ¼ge die `.env`-Datei unbedingt zu deiner `.gitignore`-Datei hinzu, damit deine API-Keys niemals auf GitHub landen!

### 2. Server starten
1.  **Berechtigungen korrigieren (einmalig):** FÃ¼hre im Projektordner `sudo chown -R $(id -u):$(id -g) .` aus, um Berechtigungsprobleme mit Docker zu vermeiden.
2.  **Container starten:** FÃ¼hre den folgenden Befehl im Terminal aus:
    ```bash
    docker-compose up -d --build
    ```
    -   `up`: Startet den Service.
    -   `-d`: Startet den Container im Hintergrund (detached mode).
    -   `--build`: Baut das Docker-Image neu, falls es Ã„nderungen gab.

### 3. API testen
Nachdem der Container lÃ¤uft, kannst du die API testen. Die erwartete Antwort bei einem leeren Server ist `[]`.

**Mit PowerShell:**
```powershell
Invoke-WebRequest -Uri "http://localhost:8000/get_memories?user_id=test" -Headers @{"X-API-Key" = "dein-super-sicherer-key-12345"}
```

**Mit cURL:**
```bash
curl -X GET "http://localhost:8000/get_memories?user_id=test" -H "X-API-Key: dein-super-sicherer-key-12345"
```

---

## âš™ï¸ Plugin Konfiguration (Valves)

Die wichtigsten Einstellungen fÃ¼r das `adaptive_memory_v4.py` Plugin findest du direkt in der `Valves`-Klasse im Code. Passe diese nach Bedarf an:

-   `extraction_provider`: WÃ¤hle "openai" oder "local".
-   `relevance_provider`: WÃ¤hle "openai", "local" oder "embedding".
-   `openai_...`: Einstellungen fÃ¼r die OpenAI API (Key wird nur benÃ¶tigt, wenn OpenAI als Provider gewÃ¤hlt ist).
-   `local_llm_...`: Einstellungen fÃ¼r deinen lokalen LLM (z.B. Ollama Chat API).
-   `local_embedding_provider`: WÃ¤hle "sentence_transformer" oder "ollama".
-   `sentence_transformer_model`: Modell fÃ¼r die `sentence-transformers` Bibliothek.
-   `ollama_embedding_...`: Einstellungen fÃ¼r die Ollama Embeddings API.
-   `memory_api_base`, `memory_api_key`: Verbindung zum Memory Server.
-   *...und weitere Thresholds und Filter.*

---

## ğŸ›£ï¸ Roadmap

Die vollstÃ¤ndige und aktuelle Roadmap wird jetzt in der Datei `ROADMAP.md` gepflegt, um diese README Ã¼bersichtlich zu halten.

[â¡ï¸ **Zur vollstÃ¤ndigen Roadmap (ROADMAP.md)**](./ROADMAP.md)

---

## ğŸ’– Credits & Lizenz

Ein groÃŸes DankeschÃ¶n geht an **gramanoid (aka diligent_chooser)**, dessen Arbeit die Inspiration fÃ¼r dieses Projekt war.

-   [Original Reddit-Post](https://www.reddit.com/r/OpenWebUI/comments/1kd0s49/adaptive_memory_v30_openwebui_plugin/)
-   [Open WebUI Plugin-Seite](https://openwebui.com/f/alexgrama7/adaptive_memory_v2)

Dieses Projekt steht unter der **[Apache License 2.0](./LICENSE)**.
Copyright Â© 2025 J. Apps

> âš ï¸ **Hinweis:** Die Lizenz gilt nur fÃ¼r dieses Memory-Projekt. Alle anderen Module des Finja-Ã–kosystems bleiben unter der MIT-Lizenz.

![Berechtigungs-Screenshot](https://github.com/JohnV2002/Finja-AI-Ecosystem/blob/main/assets/Screenshot2025-09-12.png)

---

## ğŸ†˜ Support & Kontakt

-   **E-Mail:** contact@jappshome.de
-   **Website:** [jappshome.de](https://jappshome.de)
-   **UnterstÃ¼tzung:** [Buy Me a Coffee](https://buymeacoffee.com/J.Apps)

---

**Viel Erfolg mit deinem Memory-Server!** ğŸš€âœ¨