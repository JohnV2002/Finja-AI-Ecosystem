# ğŸ“š **Cloud Memory â€“ README**

Ein leichtgewichtiger, blitzschneller **Memory-Service** fÃ¼r AI-Projekte ğŸš€  
Speichert Erinnerungen (`memories`) **pro Benutzer** und verbindet sich nahtlos mit **OpenWebUI** via dem `adaptive_memory_v4` Plugin.  

---

## ğŸ›£ï¸ Roadmap

**Prio 1.0 â€“ Hoch**
- ğŸ”„ **Embedding-Fallback beim Upload** â†’ Falls OpenAI down â†’ lokale Embeddings (all-MiniLM-L6-v2) in OpenWebUI nutzen, Relevanz prÃ¼fen, ggf. speichern oder ablehnen.
- ğŸ›¡ **Duplicate-Killer 2.0** â†’ Fuzzy-Matching + Levenshtein, vermeidet doppelte Memories.
- ğŸ§¹ **Content-Filter** â†’ Blockiert Spam, Ein-Wort-EintrÃ¤ge, irrelevante Inhalte.
- ğŸ—£ **Voice Memories** â†’ STT/TTS-Support, Audio im Memory verlinken.

**Prio 0.5 â€“ Mittel**
- ğŸ’¾ **Offline-Backup** â†’ Backups als `.tar.gz`, Download-Endpoint + SFTP-Option.
- ğŸ“¢ **Status-Emitter** â†’ Sichtbare Speicher-Events im Plugin (â€MEMORY SAVEDâ€œ, â€DUPLICATEâ€œ etc.).

**Prio 0.4 â€“ Nice-to-Have**
- ğŸ”— **Memory-Chaining** â†’ VerknÃ¼pfung Ã¤hnlicher Memories.
- ğŸ” **Private Memory Lock** â†’ Bank `"Secrets"` optional verschlÃ¼sseln.

**Prio 0.3 â€“ Zusatzfeatures**
- ğŸ—‚ **Memory-Banks** (General/Personal/Work/Jokes/Secrets) + Filterung.
- ğŸ” **Search+Ask** â†’ â€Was weiÃŸt du Ã¼ber X?â€œ â†’ Memory-Server gibt Antwort.
- ğŸ“Š **Memory-Stats Dashboard** â†’ Hits, Rejects, Duplicates, Banks.

**Prio 0.2â€“0.1 â€“ Langfristig**
- ğŸŒ **WebSocket Push** â†’ Live-Events fÃ¼r UI.
- ğŸ“œ **Memory-Story Mode** â†’ Zusammenfassung wichtiger Memories.
- â³ **Memory-Expiry** & **Auto-Prune** (optional).
- ğŸ¨ **Memory-Visualizer** â†’ UI zum DurchstÃ¶bern.

ğŸ“„ **Komplette Roadmap lesen:** [ROADMAP.md](ROADMAP.md)

---

## ğŸ–¥ï¸ 2. SERVER

### ğŸ“¦ 2.1 Dockerfile
Dieses Dockerfile erstellt einen Container, in dem der Memory-Server lÃ¤uft:  
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY memory_service.py .
RUN pip install fastapi uvicorn pydantic
EXPOSE 8000
CMD ["uvicorn", "memory_service:app", "--host", "0.0.0.0", "--port", "8000"]

# This Dockerfile sets up a FastAPI application for the memory service.
# It uses Python 3.11 on a slim base image, installs necessary dependencies,
# and runs the service on port 8000.
```
ğŸ’¡ **Was macht das?**  
- LÃ¤dt ein schlankes Python 3.11 Image  
- Kopiert unseren Memory-Server Code (`memory_service.py`) hinein  
- Installiert **FastAPI**, **Uvicorn**, **Pydantic**  
- Startet den Server auf Port **8000**  

---

### ğŸ—„ï¸ 2.2 `memory-server.py`
Der **Memory-Server**:
- Speichert Erinnerungen als **JSON-Dateien pro Benutzer**  
- Bietet eine **REST-API** an:
  - `/add_memory` â€“ einzelne Erinnerung speichern  
  - `/add_memories` â€“ mehrere Erinnerungen speichern (Batch)  
  - `/get_memories` â€“ Erinnerungen abrufen  
  - `/prune` â€“ alte EintrÃ¤ge lÃ¶schen  
  - `/backup_now` â€“ sofortiges Backup erstellen  
- Nutzt `X-API-Key` fÃ¼r Sicherheit  
- LÃ¤uft extrem ressourcenschonend â†’ ideal fÃ¼r V-Server oder Docker-Container  

---

### ğŸ³ 2.3 Setup per Docker
So startest du den Memory-Server in Docker:

1. **Docker-Image bauen**
   ```bash
   docker build -t memory-server .
   ```
2. **Container starten**
   ```bash
   docker run -d      -p 8000:8000      -v $(pwd)/user_memories:/app/user_memories      -e API_KEY=changeme-supersecretkey      --name memory-server      memory-server
   ```
3. **API testen**
   ```bash
   curl -X POST http://localhost:8000/add_memory -H "X-API-Key: changeme-supersecretkey" -H "Content-Type: application/json" -d '{"user_id": "defualt", "text": "Ich liebe Chatbots"}'
   ```

ğŸ’¡ **Tipp:**  
- Mit `-v` mountest du den Speicherordner aus dem Container auf den Host â†’ einfache Backups  
- Mit `--env` kannst du den API-Key setzen  

---

## ğŸ¤– 3. `adaptive_memory_v4` â€“ Das Plugin
Das **OpenWebUI Plugin** verbindet dein LLM mit dem Memory-Server.

### ğŸ” Was es macht
- **Holt Erinnerungen** von deinem Memory-Server (`/get_memories`) basierend auf der `user_id`
- **RelevanzprÃ¼fung**: OpenAI (oder API-kompatibles Modell) bewertet, ob eine gespeicherte Erinnerung fÃ¼r die aktuelle User-Nachricht relevant ist
- **Kontext-Injektion**: Nur relevante Erinnerungen werden dem LLM als **System-Kontext** vorangestellt
- **Memory-Extraktion**: Erkennt aus neuen Nachrichten **faktische, langfristige Infos** (z. B. Name, Vorlieben) und speichert sie
- **Dedupe**: Kein mehrfaches Speichern derselben Info
- **Guards**: Filtert irrelevante Chat-Nachrichten (â€Hiâ€œ, â€Wie gehtâ€™s?â€œ) aus

---

### ğŸ“‹ Funktionsablauf
1. **User schreibt eine Nachricht**  
2. Plugin holt alle bisherigen Memories vom Server  
3. Falls welche relevant sind â†’ werden **in den Kontext eingefÃ¼gt**  
4. Falls keine relevant sind â†’ **OpenAI-Analyse** â†’ neue Memory extrahieren  
5. **Neue relevante Memory â†’ an den Memory-Server senden**  
6. LLM antwortet selbststÃ¤ndig mit erweitertem Kontext

---

### ğŸ“œ Vorteile
- **PersÃ¶nliche Chats**: KI erinnert sich an Namen, Vorlieben, GesprÃ¤chsthemen  
- **Skalierbar**: Mehrere Benutzer, getrennte Erinnerungen  
- **Offline-fÃ¤hig**: Memory-Server kann lokal oder auf V-Server laufen  
- **Einfache Integration**: Funktioniert mit jedem API-kompatiblen LLM


---
